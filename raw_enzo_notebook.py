{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"},{"sourceId":8558594,"sourceType":"datasetVersion","datasetId":5115292,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# ## **Sign Language Recognition**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:47.300103Z\",\"iopub.execute_input\":\"2024-06-05T10:43:47.301215Z\",\"iopub.status.idle\":\"2024-06-05T10:43:47.306167Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:47.301176Z\",\"shell.execute_reply\":\"2024-06-05T10:43:47.304698Z\"}}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pickle\n\nimport os\nimport random\nimport matplotlib.pyplot as plt\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:48.585108Z\",\"iopub.execute_input\":\"2024-06-05T10:43:48.585476Z\",\"iopub.status.idle\":\"2024-06-05T10:43:48.597238Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:48.585449Z\",\"shell.execute_reply\":\"2024-06-05T10:43:48.596111Z\"}}\nCOMPETITION_PATH = '/kaggle/input/asl-signs/'\nPROCESS_DATASET_PATH = \"/kaggle/input/preprocess-dataset/preprocess_dataset.pkl\"\ndataset_path = '/kaggle/input/asl-signs/train_landmark_files'\nuser_ids = os.listdir('/kaggle/input/asl-signs/train_landmark_files')\n\n# %% [markdown]\n# ## Function to load sequence provided by Google\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:53.372576Z\",\"iopub.execute_input\":\"2024-06-05T10:43:53.372967Z\",\"iopub.status.idle\":\"2024-06-05T10:43:53.379145Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:53.372936Z\",\"shell.execute_reply\":\"2024-06-05T10:43:53.377949Z\"}}\nROWS_PER_FRAME = 543  # number of landmarks per frame\n\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:53.994557Z\",\"iopub.execute_input\":\"2024-06-05T10:43:53.994956Z\",\"iopub.status.idle\":\"2024-06-05T10:43:54.001011Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:53.994926Z\",\"shell.execute_reply\":\"2024-06-05T10:43:53.999679Z\"}}\ndef select_random_sequence():\n    usr = random.choice(user_ids)\n    usr_sqc = os.listdir(os.path.join(dataset_path,usr))\n    sqc = random.choice(usr_sqc)\n    return os.path.join(dataset_path,usr,sqc)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:54.642554Z\",\"iopub.execute_input\":\"2024-06-05T10:43:54.642930Z\",\"iopub.status.idle\":\"2024-06-05T10:43:55.136472Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:54.642904Z\",\"shell.execute_reply\":\"2024-06-05T10:43:55.135209Z\"}}\nselect_random_sequence()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:55.284390Z\",\"iopub.execute_input\":\"2024-06-05T10:43:55.284798Z\",\"iopub.status.idle\":\"2024-06-05T10:43:56.084318Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:55.284766Z\",\"shell.execute_reply\":\"2024-06-05T10:43:56.083010Z\"}}\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\npq_path = select_random_sequence()\ndf = pd.read_parquet(pq_path, columns=cols)\nprint(pq_path)\nprint(f'xmax: {np.max(df.x)}\\nymax: {np.max(df.y)}\\nxmin: {np.min(df.x)}\\nymin: {np.min(df.y)}')\n\n# %% [markdown]\n# ### **Do not run next cell (takes time)**\n# or maybe run it one time for min values\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:43:58.836753Z\",\"iopub.execute_input\":\"2024-06-05T10:43:58.837161Z\",\"iopub.status.idle\":\"2024-06-05T10:43:58.845285Z\",\"shell.execute_reply.started\":\"2024-06-05T10:43:58.837130Z\",\"shell.execute_reply\":\"2024-06-05T10:43:58.843940Z\"}}\n# maxX=[]\n# maxY=[]\n# maxZ=[]\n# for usr in user_ids:\n#     usr_sqc = os.listdir(os.path.join(dataset_path,usr))\n#     for sqc in usr_sqc:\n#         pth = os.path.join(dataset_path,usr,sqc)\n#         df = pd.read_parquet(pth, columns=['x', 'y', 'z'])\n#         maxX.append(np.max(df.x))\n#         maxY.append(np.max(df.y))\n#         maxZ.append(np.max(df.z))\n\n# print(f'max x: {np.max(maxX)}\\nmax y: {np.max(maxY)}\\nmax z: {np.max(maxZ)}')\n\n'''\noutputs:\n\nmax x: 2.9205052852630615\nmax y: 3.572496175765991\nmax z: 4.796591758728027\n'''\n\n# %% [markdown]\n# ### **Prepocessing**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:00.331774Z\",\"iopub.execute_input\":\"2024-06-05T10:44:00.332278Z\",\"iopub.status.idle\":\"2024-06-05T10:44:00.342545Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:00.332241Z\",\"shell.execute_reply\":\"2024-06-05T10:44:00.341342Z\"}}\n# lips idx\nLIPS_IDXS0 = np.array([\n        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n    ])\n\n# left hand, by taking account face from 0 to 468\nLEFT_HAND_IDXS0 = np.arange(468,489)\nRIGHT_HAND_IDXS0 = np.arange(522,543)\nLEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\nRIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n\nREDUCED_LANDMARKS = np.sort(np.concatenate([LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, LEFT_POSE_IDXS0, RIGHT_POSE_IDXS0]))\nprint(REDUCED_LANDMARKS)\n\n# %% [markdown]\n# **Note** positions kept as wanted\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:01.731342Z\",\"iopub.execute_input\":\"2024-06-05T10:44:01.731727Z\",\"iopub.status.idle\":\"2024-06-05T10:44:02.280711Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:01.731699Z\",\"shell.execute_reply\":\"2024-06-05T10:44:02.279323Z\"}}\n# function to replace NaN and normalize columns \npq_path = select_random_sequence() # only first sequence of user here\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\nsqc_df = pd.read_parquet(pq_path, columns=cols)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:02.316742Z\",\"iopub.execute_input\":\"2024-06-05T10:44:02.317894Z\",\"iopub.status.idle\":\"2024-06-05T10:44:02.401664Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:02.317847Z\",\"shell.execute_reply\":\"2024-06-05T10:44:02.400656Z\"}}\ndef normalize_sequence(sequence_dataframe):\n    '''\n        function to normalize coordinates columns (x,y) per frame, also replace NaN values by column mean\n        sequence_dataset is a pandas dataframe containing a sequence of an user\n    '''\n\n\n\n    frame_sqc_idx = sqc_df.frame.unique()\n    normalized_df = pd.DataFrame()\n\n    for frame in frame_sqc_idx:\n        frame_df = sqc_df[sqc_df.frame == frame]\n        frame_df1 = frame_df.copy()\n        \n        na_x = frame_df['x'].fillna(0.0)\n        na_y = frame_df['y'].fillna(0.0)\n\n        x_norm = (na_x-np.min(na_x))/(np.max(na_x)-np.min(na_x))\n        y_norm = (na_y-np.min(na_y))/(np.max(na_y)-np.min(na_y))\n\n        frame_df1.x, frame_df1.y = x_norm, y_norm\n        normalized_df = pd.concat([normalized_df, frame_df1])\n    \n    return normalized_df\n\nnormalized_df=normalize_sequence(sqc_df)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:03.806858Z\",\"iopub.execute_input\":\"2024-06-05T10:44:03.807250Z\",\"iopub.status.idle\":\"2024-06-05T10:44:03.816376Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:03.807219Z\",\"shell.execute_reply\":\"2024-06-05T10:44:03.815094Z\"}}\nlen(sqc_df), len(normalized_df)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:04.373501Z\",\"iopub.execute_input\":\"2024-06-05T10:44:04.373892Z\",\"iopub.status.idle\":\"2024-06-05T10:44:05.135957Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:04.373862Z\",\"shell.execute_reply\":\"2024-06-05T10:44:05.134769Z\"}}\nv = load_relevant_data_subset(select_random_sequence())\nprint(v.shape)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:05.138250Z\",\"iopub.execute_input\":\"2024-06-05T10:44:05.138616Z\",\"iopub.status.idle\":\"2024-06-05T10:44:05.150317Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:05.138572Z\",\"shell.execute_reply\":\"2024-06-05T10:44:05.149099Z\"}}\ndef normalize_loaded_sequence(loaded_sqc):\n    '''\n        Function to normalize using min-max normalization. \n        Normalization is calculated over all points, but only relevants landmarks points are returned\n        This function also replaces NaN by 0\n    '''\n    normalized_sqc = np.zeros((loaded_sqc.shape[0], len(REDUCED_LANDMARKS), 2))\n    \n    for frm_idx in range(loaded_sqc.shape[0]):\n        frame_array = loaded_sqc[frm_idx]\n        \n        na_x = np.nan_to_num(frame_array[:,0], nan=0.0)\n        na_y = np.nan_to_num(frame_array[:,1], nan=0.0)\n\n\n        x_norm = (na_x-np.min(na_x))/(np.max(na_x)-np.min(na_x))\n        y_norm = (na_y-np.min(na_y))/(np.max(na_y)-np.min(na_y))\n\n        normalized_sqc[frm_idx,:,0],  normalized_sqc[frm_idx,:,1] = x_norm[REDUCED_LANDMARKS], y_norm[REDUCED_LANDMARKS]\n    \n    return normalized_sqc\n\nn_v = normalize_loaded_sequence(v)\nprint(n_v.shape)\nprint(np.max(n_v[0,:,0]), np.min(n_v[0,:,0]))\n\n# %% [markdown]\n# **Note** at this step I have a normalized tensor built after loading data\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:05.983438Z\",\"iopub.execute_input\":\"2024-06-05T10:44:05.983846Z\",\"iopub.status.idle\":\"2024-06-05T10:44:05.989656Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:05.983812Z\",\"shell.execute_reply\":\"2024-06-05T10:44:05.988307Z\"}}\ndef get_data(sqc_path):\n    data = load_relevant_data_subset(sqc_path)\n    data = normalize_loaded_sequence(data)\n    return data\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:06.447559Z\",\"iopub.execute_input\":\"2024-06-05T10:44:06.448394Z\",\"iopub.status.idle\":\"2024-06-05T10:44:07.051291Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:06.448354Z\",\"shell.execute_reply\":\"2024-06-05T10:44:07.050309Z\"}}\nd = get_data(select_random_sequence())\nd.shape\n# print(vv.shape)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:08.681038Z\",\"iopub.execute_input\":\"2024-06-05T10:44:08.681428Z\",\"iopub.status.idle\":\"2024-06-05T10:44:09.766515Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:08.681396Z\",\"shell.execute_reply\":\"2024-06-05T10:44:09.765291Z\"}}\npq_path = select_random_sequence() # only first sequence of user here\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\nsqc_df = pd.read_parquet(pq_path, columns=cols)\n\nvv = get_data(pq_path)\n\nn_df = normalize_sequence(sqc_df)\nframe_df0 = n_df[n_df.frame == n_df.frame.unique()[0]]\nframe_df1 = n_df[n_df.frame == n_df.frame.unique()[-1]]\n\nX0 = frame_df0.x\nY0= frame_df0.y\n\nX1 = frame_df1.x\nY1= frame_df1.y\n\nplt.figure(figsize=(8,10))\nplt.subplot(1,2,1)\nplt.scatter(X0,-Y0)\nplt.scatter(vv[0,:,0],-vv[0,:,1], s=3, c='r')\n\nplt.subplot(1,2,2)\nplt.scatter(X1,-Y1)\nplt.scatter(vv[-1,:,0],-vv[-1,:,1], s=3, c='r')\n\nplt.title(pq_path)\nplt.show()\n\n# %% [markdown]\n# #### **Note** \n# Normalization using min-max change position of point when using less (but most important) landmarks, is it normal as we used less points.\n# But movement keep the same\n# \n# - RNN or LSTM can be a good simple approach for starting (it can be adapted for Time Series)\n\n# %% [markdown]\n# #### **TODO**\n# * thing about data augmentation\n# * try to use coatnet -> need to input data with same shape\n# * padding ?\n#     - issue with padding is that we have sequence with much more frames than other, maybe reduce thoses sequences and padding for small sequences\n#     - goal: have se\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:12.466284Z\",\"iopub.execute_input\":\"2024-06-05T10:44:12.466713Z\",\"iopub.status.idle\":\"2024-06-05T10:44:12.742245Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:12.466680Z\",\"shell.execute_reply\":\"2024-06-05T10:44:12.740966Z\"}}\ntrain_path = '/kaggle/input/asl-signs/train.csv'\ntrain = pd.read_csv(train_path)\ntrain.head()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:14.272783Z\",\"iopub.execute_input\":\"2024-06-05T10:44:14.274196Z\",\"iopub.status.idle\":\"2024-06-05T10:44:14.282030Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:14.274154Z\",\"shell.execute_reply\":\"2024-06-05T10:44:14.280682Z\"}}\nlen(train)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:14.661114Z\",\"iopub.execute_input\":\"2024-06-05T10:44:14.661482Z\",\"iopub.status.idle\":\"2024-06-05T10:44:14.668318Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:14.661451Z\",\"shell.execute_reply\":\"2024-06-05T10:44:14.667266Z\"}}\ntrain.columns\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:15.012442Z\",\"iopub.execute_input\":\"2024-06-05T10:44:15.012840Z\",\"iopub.status.idle\":\"2024-06-05T10:44:15.030424Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:15.012809Z\",\"shell.execute_reply\":\"2024-06-05T10:44:15.029038Z\"}}\ntrain.sign.unique()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:15.707129Z\",\"iopub.execute_input\":\"2024-06-05T10:44:15.707504Z\",\"iopub.status.idle\":\"2024-06-05T10:44:15.716378Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:15.707474Z\",\"shell.execute_reply\":\"2024-06-05T10:44:15.715226Z\"}}\ntrain.participant_id.unique(), len(train.participant_id.unique())\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:17.051300Z\",\"iopub.execute_input\":\"2024-06-05T10:44:17.051690Z\",\"iopub.status.idle\":\"2024-06-05T10:44:17.091264Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:17.051658Z\",\"shell.execute_reply\":\"2024-06-05T10:44:17.090057Z\"}}\nd=dict(train.sign.value_counts(dropna=True))\nprint(train.sign.value_counts(dropna=True).mean())\nprint(train.sign.value_counts(dropna=True).std())\nprint(train.sign.value_counts(dropna=True).max())\nprint(train.sign.value_counts(dropna=True).min())\n\n# word distribution is not too expended\n# any words have close occurences\n\n# %% [markdown]\n# #### **Some notes:**\n# * each parquet contains markers position [x y z] and type (face, left_hand, pose, right_hand) for different frame\n# * train dataset is composed of image path, participant id (folder name of parquet file) sequence id (filename) and word said\n# * one sequence = numerous frames = 1 word\n# * every frame has data for each type, but it is possible that one type has no value in a frame, it is setted to NaN\n# \n# **Goal**: using hand position, be able to understand word said in the sequence\n# * classification between 250 words using positions of body parts in video\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:18.105149Z\",\"iopub.execute_input\":\"2024-06-05T10:44:18.105946Z\",\"iopub.status.idle\":\"2024-06-05T10:44:18.118502Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:18.105911Z\",\"shell.execute_reply\":\"2024-06-05T10:44:18.117257Z\"}}\nimport json\n \n# Opening JSON file\nf = open('/kaggle/input/asl-signs/sign_to_prediction_index_map.json')\n \n# returns JSON object as \n# a dictionary\nWORD2IDX = json.load(f)\nprint(len(WORD2IDX), WORD2IDX)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:20.329288Z\",\"iopub.execute_input\":\"2024-06-05T10:44:20.329686Z\",\"iopub.status.idle\":\"2024-06-05T10:44:20.342352Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:20.329652Z\",\"shell.execute_reply\":\"2024-06-05T10:44:20.341110Z\"}}\ntrain_words = train.sign.unique()\nprint(len(train_words))\n# same length as sign to prediction index json\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:20.995567Z\",\"iopub.execute_input\":\"2024-06-05T10:44:20.995977Z\",\"iopub.status.idle\":\"2024-06-05T10:44:21.008107Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:20.995947Z\",\"shell.execute_reply\":\"2024-06-05T10:44:21.006855Z\"}}\nrandom_word = random.choice(train.sign.unique())\nprint(f'idx for <{random_word}> is <{WORD2IDX[random_word]}>')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:21.602375Z\",\"iopub.execute_input\":\"2024-06-05T10:44:21.602774Z\",\"iopub.status.idle\":\"2024-06-05T10:44:21.611685Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:21.602738Z\",\"shell.execute_reply\":\"2024-06-05T10:44:21.610633Z\"}}\ntrain.path\n\n# %% [markdown]\n# ### Custom Dataset class\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:24.130568Z\",\"iopub.execute_input\":\"2024-06-05T10:44:24.130952Z\",\"iopub.status.idle\":\"2024-06-05T10:44:24.136955Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:24.130923Z\",\"shell.execute_reply\":\"2024-06-05T10:44:24.135667Z\"}}\nall_sqc_path = train.path\nprint(len(all_sqc_path))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:24.506726Z\",\"iopub.execute_input\":\"2024-06-05T10:44:24.507114Z\",\"iopub.status.idle\":\"2024-06-05T10:44:26.586588Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:24.507085Z\",\"shell.execute_reply\":\"2024-06-05T10:44:26.585479Z\"}}\nmyList = []\n\nfor i in range(5):\n    sq1 = select_random_sequence()\n    word = train[train.path == sq1[24:]].sign.values[0]\n    mydata = get_data(sq1)\n    print(mydata.shape)\n    myList.append((mydata, word))\n\n\n\n# %% [markdown]\n# ### **Don't run following cell, it creates *preprocess_dataset***\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:30.061968Z\",\"iopub.execute_input\":\"2024-06-05T10:44:30.062379Z\",\"iopub.status.idle\":\"2024-06-05T10:44:30.067318Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:30.062348Z\",\"shell.execute_reply\":\"2024-06-05T10:44:30.066067Z\"}}\n# Do not run !\n\n# processed_dataset = []\n# for idx,path in enumerate(train.path):\n#     sequence_path = os.path.join(COMPETITION_PATH, path)\n#     word = train[train.path == path].sign.values[0]\n#     processed_sqc = get_data(sequence_path)\n    \n#     processed_dataset.append((processed_sqc, word))\n    \n#     if idx%200 == 0:\n#         print(processed_sqc.shape, word)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:30.622836Z\",\"iopub.execute_input\":\"2024-06-05T10:44:30.623520Z\",\"iopub.status.idle\":\"2024-06-05T10:44:30.627981Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:30.623487Z\",\"shell.execute_reply\":\"2024-06-05T10:44:30.626687Z\"}}\n# to save dataset\n# with open(\"preprocess_dataset.pkl\", \"wb\") as fp:   #Pickling\n#     pickle.dump(processed_dataset, fp)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:44:32.174589Z\",\"iopub.execute_input\":\"2024-06-05T10:44:32.174983Z\",\"iopub.status.idle\":\"2024-06-05T10:45:34.529589Z\",\"shell.execute_reply.started\":\"2024-06-05T10:44:32.174953Z\",\"shell.execute_reply\":\"2024-06-05T10:45:34.528517Z\"}}\n# to load dataset\nwith open(PROCESS_DATASET_PATH, \"rb\") as fp:   # Unpickling\n    dataset = pickle.load(fp)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:29.058659Z\",\"iopub.execute_input\":\"2024-06-05T10:47:29.059772Z\",\"iopub.status.idle\":\"2024-06-05T10:47:29.065916Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:29.059730Z\",\"shell.execute_reply\":\"2024-06-05T10:47:29.064902Z\"}}\ndataset[0][1]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:29.458826Z\",\"iopub.execute_input\":\"2024-06-05T10:47:29.459341Z\",\"iopub.status.idle\":\"2024-06-05T10:47:29.466949Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:29.459300Z\",\"shell.execute_reply\":\"2024-06-05T10:47:29.465623Z\"}}\nlen(dataset)\n\n# %% [markdown]\n# ### **Custom class and Dataloader**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:30.435169Z\",\"iopub.execute_input\":\"2024-06-05T10:47:30.435567Z\",\"iopub.status.idle\":\"2024-06-05T10:47:33.551577Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:30.435535Z\",\"shell.execute_reply\":\"2024-06-05T10:47:33.550433Z\"}}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:33.553539Z\",\"iopub.execute_input\":\"2024-06-05T10:47:33.554089Z\",\"iopub.status.idle\":\"2024-06-05T10:47:33.561321Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:33.554058Z\",\"shell.execute_reply\":\"2024-06-05T10:47:33.560111Z\"}}\nclass ISLR(Dataset):\n    def __init__(self, dataset, split):\n        self.split = split\n        self.dataset = dataset\n        \n        if split == 'train':\n            self.islr_dataset = dataset[:int(0.8*len(dataset))]\n        elif split =='test':\n            self.islr_dataset = dataset[int(0.8*len(dataset)):]\n        \n    def __len__(self):\n        return len(self.islr_dataset)\n    \n    def __getitem__(self, index):\n        sample = self.islr_dataset[index]\n        features = torch.FloatTensor(sample[0])\n        target = WORD2IDX[sample[1]]\n        \n        return features, target\n    \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:33.563109Z\",\"iopub.execute_input\":\"2024-06-05T10:47:33.563529Z\",\"iopub.status.idle\":\"2024-06-05T10:47:33.579039Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:33.563492Z\",\"shell.execute_reply\":\"2024-06-05T10:47:33.577913Z\"}}\ntestset = ISLR(dataset, split='test')\ntrainset = ISLR(dataset, split='train')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:37.704707Z\",\"iopub.execute_input\":\"2024-06-05T10:47:37.705661Z\",\"iopub.status.idle\":\"2024-06-05T10:47:37.712262Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:37.705624Z\",\"shell.execute_reply\":\"2024-06-05T10:47:37.711121Z\"}}\nlen(dataset)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:38.113022Z\",\"iopub.execute_input\":\"2024-06-05T10:47:38.113402Z\",\"iopub.status.idle\":\"2024-06-05T10:47:38.120379Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:38.113373Z\",\"shell.execute_reply\":\"2024-06-05T10:47:38.119209Z\"}}\nlen(testset)+len(trainset)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:39.892430Z\",\"iopub.execute_input\":\"2024-06-05T10:47:39.892841Z\",\"iopub.status.idle\":\"2024-06-05T10:47:39.898417Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:39.892809Z\",\"shell.execute_reply\":\"2024-06-05T10:47:39.897067Z\"}}\n# batch =[[torch.tensor([1833, 3205,  467,  342, 4165,   31, 49,  803]), torch.tensor([1])],\n#         [torch.tensor([1833, 3205,  467,  342, 49,  803]), torch.tensor([2])],\n#         [torch.tensor([1833, 3205,  467,  342, 4165,   31, 49,  803,52,54]), torch.tensor([1])]]\n# def custom_collate(batch):\n#     padded_batch=[]\n#     labels=[]\n#     for sentence,label in batch:\n#         # print(sentence.tolist())\n\n#         listSentence = sentence.tolist()\n#         max_len = max(len(sentence.tolist()) for sentence,label in batch)\n#         # print(listSentence)\n#         padded_sentence=listSentence+[5001]*(max_len-len(listSentence))\n#         # print(max_len)\n#         padded_batch.append(padded_sentence)\n#         labels.append(label)\n\n#     return torch.tensor(padded_batch), torch.tensor(labels)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:40.215104Z\",\"iopub.execute_input\":\"2024-06-05T10:47:40.215491Z\",\"iopub.status.idle\":\"2024-06-05T10:47:40.223020Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:40.215462Z\",\"shell.execute_reply\":\"2024-06-05T10:47:40.221674Z\"}}\n\ndef custom_collate_fn(batch):\n    padded_batch = []\n    labels= []\n\n    max_frame = max(len(sequence) for sequence,_ in batch)\n#     print(max_frame)\n    for sequence, label in batch:\n        padding_array = -np.ones(((max_frame-len(sequence)), len(REDUCED_LANDMARKS), 2))\n        padded_sequence = sequence.tolist()+padding_array.tolist()\n\n        padded_batch.append(padded_sequence)\n        labels.append(label)\n\n\n    return torch.tensor(padded_batch), torch.tensor(labels)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:42.178482Z\",\"iopub.execute_input\":\"2024-06-05T10:47:42.179463Z\",\"iopub.status.idle\":\"2024-06-05T10:47:42.184812Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:42.179425Z\",\"shell.execute_reply\":\"2024-06-05T10:47:42.183512Z\"}}\ntrain_loader = DataLoader(trainset, batch_size=16, collate_fn=custom_collate_fn, shuffle=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:44.341411Z\",\"iopub.execute_input\":\"2024-06-05T10:47:44.341832Z\",\"iopub.status.idle\":\"2024-06-05T10:47:44.378227Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:44.341797Z\",\"shell.execute_reply\":\"2024-06-05T10:47:44.376993Z\"}}\ncustom_it = enumerate(train_loader)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:45.439496Z\",\"iopub.execute_input\":\"2024-06-05T10:47:45.440509Z\",\"iopub.status.idle\":\"2024-06-05T10:47:46.190459Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:45.440471Z\",\"shell.execute_reply\":\"2024-06-05T10:47:46.189333Z\"}}\nidx,(sqc,lb)=next(custom_it)\nprint(sqc.shape, lb)\n\n# %% [markdown]\n# ### **Model architecture**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:49.742803Z\",\"iopub.execute_input\":\"2024-06-05T10:47:49.743190Z\",\"iopub.status.idle\":\"2024-06-05T10:47:49.748166Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:49.743161Z\",\"shell.execute_reply\":\"2024-06-05T10:47:49.747023Z\"}}\n# class SignLanguageModel(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n#         super(SignLanguageModel, self).__init__()\n#         self.num_layers = num_layers\n#         self.hidden_dim = hidden_dim\n#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n#         self.fc = nn.Linear(hidden_dim, output_dim)\n\n#     def forward(self, x):\n#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n#         out, _ = self.lstm(x, (h0, c0))\n#         out = self.fc(out[:, -1, :])\n#         return out\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:47:50.117480Z\",\"iopub.execute_input\":\"2024-06-05T10:47:50.118215Z\",\"iopub.status.idle\":\"2024-06-05T10:47:50.131408Z\",\"shell.execute_reply.started\":\"2024-06-05T10:47:50.118176Z\",\"shell.execute_reply\":\"2024-06-05T10:47:50.130021Z\"}}\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, output_dim, n_landmarks):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Linear(input_dim*n_landmarks, hidden_dim)\n        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        \n        batch_size, n_frames, n_landmarks, input_dim = x.shape\n        pad_mask = self.sequence_mask(x)\n        pad_mask = pad_mask.to(device)\n        \n        \n        # Flatten n_landmarks and input_dim for embedding\n        x = x.view(batch_size, n_frames, -1)\n        assert not torch.isnan(x).any(), \"NaN in input embedding\"\n        \n        \n        x = self.embedding(x)\n        assert not torch.isnan(x).any(), \"NaN  after embedding\"\n        \n        \n        x = x.permute(1, 0, 2)  # Transformer expects sequence length first\n        \n        assert not torch.isnan(x).any(), \"NaN in input to TransformerEncoder\"\n        assert not torch.isnan(pad_mask).any(), \"NaN in mask\"\n        \n        for param in self.transformer_encoder.parameters():\n            assert not torch.isnan(param).any(), \"NaN in TransformerEncoderLayer parameters\"\n                \n        \n        transformer_out = self.transformer_encoder(x,src_key_padding_mask=pad_mask)\n        assert not torch.isnan(transformer_out).any(), \"NaN in transformer out\"\n        \n        \n        out = self.fc(transformer_out[-1, :, :])\n        assert not torch.isnan(out).any(), \"NaN in final output\"\n        \n        \n        return out\n    \n    def sequence_mask(self, sequence):\n        lengths = [self.valid_len(padded_sequence) for padded_sequence in sequence]\n        \n        mask = torch.zeros(sequence.size()[:2], dtype=torch.bool)  # shape: [batch_size, n_frames]\n        for i, length in enumerate(lengths):\n            mask[i, :length] = 1\n        \n        mask = ~mask # True values are ignored\n        return mask\n\n        \n    def valid_len(self, padded_sequence):\n        for idx, frame in  enumerate(padded_sequence):\n            if -1 in frame:\n                break\n\n        return idx+1\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:24.126579Z\",\"iopub.execute_input\":\"2024-06-05T10:48:24.127012Z\",\"iopub.status.idle\":\"2024-06-05T10:48:24.147694Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:24.126980Z\",\"shell.execute_reply\":\"2024-06-05T10:48:24.146116Z\"}}\n# Exemple d'utilisation\ninput_dim = 2  # (x, y)\nnum_heads = 4\nnum_layers = 2\nhidden_dim = 64\noutput_dim = 250  # nombre de mots\nn_landmarks = 92\n\nmodel = TransformerModel(input_dim=input_dim,\n                         num_heads=num_heads,\n                         num_layers=num_layers,\n                         hidden_dim=hidden_dim,\n                         output_dim=output_dim,\n                         n_landmarks=n_landmarks)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\nprint(model)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:26.311018Z\",\"iopub.execute_input\":\"2024-06-05T10:48:26.311406Z\",\"iopub.status.idle\":\"2024-06-05T10:48:26.612210Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:26.311376Z\",\"shell.execute_reply\":\"2024-06-05T10:48:26.611080Z\"}}\nv = model(sqc)\n[torch.argmax(vi, ) for vi in v]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:33.090242Z\",\"iopub.execute_input\":\"2024-06-05T10:48:33.090686Z\",\"iopub.status.idle\":\"2024-06-05T10:48:33.104119Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:33.090645Z\",\"shell.execute_reply\":\"2024-06-05T10:48:33.102841Z\"}}\nk_pred, idx = torch.topk(v, 3, dim=1)\nidx\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:42.988869Z\",\"iopub.execute_input\":\"2024-06-05T10:48:42.989280Z\",\"iopub.status.idle\":\"2024-06-05T10:48:42.996540Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:42.989251Z\",\"shell.execute_reply\":\"2024-06-05T10:48:42.995089Z\"}}\ndef valid_len(padded_sequence):\n    for idx, frame in  enumerate(padded_sequence):\n        if -1 in frame:\n            break\n    \n    return idx+1\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:44.898901Z\",\"iopub.execute_input\":\"2024-06-05T10:48:44.899274Z\",\"iopub.status.idle\":\"2024-06-05T10:48:44.906002Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:44.899247Z\",\"shell.execute_reply\":\"2024-06-05T10:48:44.904639Z\"}}\nsqc.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:48:59.763219Z\",\"iopub.execute_input\":\"2024-06-05T10:48:59.763618Z\",\"iopub.status.idle\":\"2024-06-05T10:48:59.784328Z\",\"shell.execute_reply.started\":\"2024-06-05T10:48:59.763571Z\",\"shell.execute_reply\":\"2024-06-05T10:48:59.783075Z\"}}\nlengths = [valid_len(padded_sequence) for padded_sequence in sqc]\nlengths\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:49:00.997061Z\",\"iopub.execute_input\":\"2024-06-05T10:49:00.997456Z\",\"iopub.status.idle\":\"2024-06-05T10:49:01.006394Z\",\"shell.execute_reply.started\":\"2024-06-05T10:49:00.997425Z\",\"shell.execute_reply\":\"2024-06-05T10:49:01.004775Z\"}}\nmask = torch.zeros(sqc.size()[:2], dtype=torch.bool)  # shape: [batch_size, n_frames, 1]\nprint(mask.shape)\nfor i, length in enumerate(lengths):\n    mask[i, :length] = 1\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:49:04.658486Z\",\"iopub.execute_input\":\"2024-06-05T10:49:04.658919Z\",\"iopub.status.idle\":\"2024-06-05T10:49:04.667989Z\",\"shell.execute_reply.started\":\"2024-06-05T10:49:04.658885Z\",\"shell.execute_reply\":\"2024-06-05T10:49:04.666815Z\"}}\nmask[1]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:49:07.190488Z\",\"iopub.execute_input\":\"2024-06-05T10:49:07.191511Z\",\"iopub.status.idle\":\"2024-06-05T10:49:07.200716Z\",\"shell.execute_reply.started\":\"2024-06-05T10:49:07.191468Z\",\"shell.execute_reply\":\"2024-06-05T10:49:07.199486Z\"}}\n# mask = mask.sum(dim=1) == 0\nmask = ~mask\nmask[1]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:49:09.768696Z\",\"iopub.execute_input\":\"2024-06-05T10:49:09.769108Z\",\"iopub.status.idle\":\"2024-06-05T10:49:09.776260Z\",\"shell.execute_reply.started\":\"2024-06-05T10:49:09.769078Z\",\"shell.execute_reply\":\"2024-06-05T10:49:09.775086Z\"}}\nmask.shape\n\n# %% [markdown]\n# ### **Training Phase**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:49:15.978399Z\",\"iopub.execute_input\":\"2024-06-05T10:49:15.978828Z\",\"iopub.status.idle\":\"2024-06-05T10:49:17.566358Z\",\"shell.execute_reply.started\":\"2024-06-05T10:49:15.978797Z\",\"shell.execute_reply\":\"2024-06-05T10:49:17.565133Z\"}}\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-05T10:50:22.939269Z\",\"iopub.execute_input\":\"2024-06-05T10:50:22.939866Z\",\"iopub.status.idle\":\"2024-06-05T10:50:22.945242Z\",\"shell.execute_reply.started\":\"2024-06-05T10:50:22.939830Z\",\"shell.execute_reply\":\"2024-06-05T10:50:22.944002Z\"}}\n# num_epochs = 15\n\n# dataloader = train_loader\n\n# for epoch in range(num_epochs):\n\n#     print(f'Epoch {epoch}/{num_epochs - 1}')\n#     print('-' * 10)\n    \n#     model.train()\n#     running_loss = 0.0\n#     running_corrects = 0\n\n#     for sequence, label in dataloader:\n#         sequence, label = sequence.to(device), label.to(device)\n#         optimizer.zero_grad()\n\n#         target = label\n        \n#         outputs = model(sequence)\n\n#         predictions = torch.argmax(outputs, dim=1) # get index of max word\n\n#         # Compute the loss, gradients, and update optimizer\n#         loss = loss_function(outputs, target)\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n#         running_corrects += torch.sum(predictions == label)\n\n#     exp_lr_scheduler.step()\n\n#     epoch_loss = running_loss / len(dataloader)\n#     epoch_acc = running_corrects.double() /len(dataloader)\n\n#     print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n# %% [markdown]\n# #### **Analysis Ideas**\n# \n# * class embalencement (count words for each element in train dataset)\n# * size analysis (lenght of sequence, linked to words ?)\n# * position ranges (x y z)\n# * number of sequence per participant \n# * train dataset will be splitted for train test val","metadata":{"_uuid":"aa66b353-d400-4ec2-9650-56c3199cfdb9","_cell_guid":"7686b096-5e70-4e04-a0e1-82f45246b6da","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}