{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"},{"sourceId":8558594,"sourceType":"datasetVersion","datasetId":5115292,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# ## **Sign Language Recognition**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:44.279929Z\",\"iopub.execute_input\":\"2024-06-13T18:23:44.280328Z\",\"iopub.status.idle\":\"2024-06-13T18:23:44.688991Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:44.280296Z\",\"shell.execute_reply\":\"2024-06-13T18:23:44.687928Z\"}}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pickle\n\nimport os\nimport random\nimport matplotlib.pyplot as plt\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:44.690664Z\",\"iopub.execute_input\":\"2024-06-13T18:23:44.691066Z\",\"iopub.status.idle\":\"2024-06-13T18:23:44.699020Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:44.691040Z\",\"shell.execute_reply\":\"2024-06-13T18:23:44.698007Z\"}}\nCOMPETITION_PATH = '/kaggle/input/asl-signs/'\nPROCESS_DATASET_PATH = \"/kaggle/input/preprocess-dataset/preprocess_dataset.pkl\"\ndataset_path = '/kaggle/input/asl-signs/train_landmark_files'\nuser_ids = os.listdir('/kaggle/input/asl-signs/train_landmark_files')\n\n# %% [markdown]\n# ## Function to load sequence provided by Google\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:44.700138Z\",\"iopub.execute_input\":\"2024-06-13T18:23:44.700445Z\",\"iopub.status.idle\":\"2024-06-13T18:23:44.708582Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:44.700421Z\",\"shell.execute_reply\":\"2024-06-13T18:23:44.707664Z\"}}\nROWS_PER_FRAME = 543  # number of landmarks per frame\n\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:44.710919Z\",\"iopub.execute_input\":\"2024-06-13T18:23:44.711202Z\",\"iopub.status.idle\":\"2024-06-13T18:23:44.719970Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:44.711178Z\",\"shell.execute_reply\":\"2024-06-13T18:23:44.718987Z\"}}\ndef select_random_sequence():\n    usr = random.choice(user_ids)\n    usr_sqc = os.listdir(os.path.join(dataset_path,usr))\n    sqc = random.choice(usr_sqc)\n    return os.path.join(dataset_path,usr,sqc)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:44.720968Z\",\"iopub.execute_input\":\"2024-06-13T18:23:44.721260Z\",\"iopub.status.idle\":\"2024-06-13T18:23:45.323863Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:44.721201Z\",\"shell.execute_reply\":\"2024-06-13T18:23:45.322884Z\"}}\nselect_random_sequence()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:45.325477Z\",\"iopub.execute_input\":\"2024-06-13T18:23:45.325836Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.006882Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:45.325803Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.005723Z\"}}\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\npq_path = select_random_sequence()\ndf = pd.read_parquet(pq_path, columns=cols)\nprint(pq_path)\nprint(f'xmax: {np.max(df.x)}\\nymax: {np.max(df.y)}\\nxmin: {np.min(df.x)}\\nymin: {np.min(df.y)}')\n\n# %% [markdown]\n# ### **Do not run next cell (takes time)**\n# or maybe run it one time for min values\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.008459Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.009194Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.017653Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.009153Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.016534Z\"}}\n# maxX=[]\n# maxY=[]\n# maxZ=[]\n# for usr in user_ids:\n#     usr_sqc = os.listdir(os.path.join(dataset_path,usr))\n#     for sqc in usr_sqc:\n#         pth = os.path.join(dataset_path,usr,sqc)\n#         df = pd.read_parquet(pth, columns=['x', 'y', 'z'])\n#         maxX.append(np.max(df.x))\n#         maxY.append(np.max(df.y))\n#         maxZ.append(np.max(df.z))\n\n# print(f'max x: {np.max(maxX)}\\nmax y: {np.max(maxY)}\\nmax z: {np.max(maxZ)}')\n\n'''\noutputs:\n\nmax x: 2.9205052852630615\nmax y: 3.572496175765991\nmax z: 4.796591758728027\n'''\n\n# %% [markdown]\n# ### **Prepocessing**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.019459Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.019759Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.030484Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.019735Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.029306Z\"}}\n# lips idx\nLIPS_IDXS0 = np.array([\n        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n    ])\n\n# left hand, by taking account face from 0 to 468\nLEFT_HAND_IDXS0 = np.arange(468,489)\nRIGHT_HAND_IDXS0 = np.arange(522,543)\nLEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\nRIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n\nREDUCED_LANDMARKS = np.sort(np.concatenate([LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, LEFT_POSE_IDXS0, RIGHT_POSE_IDXS0]))\nprint(REDUCED_LANDMARKS)\n\n# %% [markdown]\n# **Note** positions kept as wanted\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.032308Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.032660Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.472143Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.032626Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.471181Z\"}}\n# function to replace NaN and normalize columns \npq_path = select_random_sequence() # only first sequence of user here\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\nsqc_df = pd.read_parquet(pq_path, columns=cols)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.477143Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.477456Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.579771Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.477429Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.578620Z\"}}\ndef normalize_sequence(sequence_dataframe):\n    '''\n        function to normalize coordinates columns (x,y) per frame, also replace NaN values by column mean\n        sequence_dataset is a pandas dataframe containing a sequence of an user\n    '''\n\n\n\n    frame_sqc_idx = sqc_df.frame.unique()\n    normalized_df = pd.DataFrame()\n\n    for frame in frame_sqc_idx:\n        frame_df = sqc_df[sqc_df.frame == frame]\n        frame_df1 = frame_df.copy()\n        \n        na_x = frame_df['x'].fillna(0.0)\n        na_y = frame_df['y'].fillna(0.0)\n\n        x_norm = (na_x-np.min(na_x))/(np.max(na_x)-np.min(na_x))\n        y_norm = (na_y-np.min(na_y))/(np.max(na_y)-np.min(na_y))\n\n        frame_df1.x, frame_df1.y = x_norm, y_norm\n        normalized_df = pd.concat([normalized_df, frame_df1])\n    \n    return normalized_df\n\nnormalized_df=normalize_sequence(sqc_df)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.581250Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.581920Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.588879Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.581881Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.587859Z\"}}\nlen(sqc_df), len(normalized_df)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.590414Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.591039Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.629321Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.591001Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.628281Z\"}}\nv = load_relevant_data_subset(select_random_sequence())\nprint(v.shape)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.631133Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.631544Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.648160Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.631507Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.646303Z\"}}\ndef normalize_loaded_sequence(loaded_sqc):\n    '''\n        Function to normalize using min-max normalization. \n        Normalization is calculated over all points, but only relevants landmarks points are returned\n        This function also replaces NaN by 0\n    '''\n    normalized_sqc = np.zeros((loaded_sqc.shape[0], len(REDUCED_LANDMARKS), 2))\n    \n    for frm_idx in range(loaded_sqc.shape[0]):\n        frame_array = loaded_sqc[frm_idx]\n        \n        na_x = np.nan_to_num(frame_array[:,0], nan=0.0)\n        na_y = np.nan_to_num(frame_array[:,1], nan=0.0)\n\n\n        x_norm = (na_x-np.min(na_x))/(np.max(na_x)-np.min(na_x))\n        y_norm = (na_y-np.min(na_y))/(np.max(na_y)-np.min(na_y))\n\n        normalized_sqc[frm_idx,:,0],  normalized_sqc[frm_idx,:,1] = x_norm[REDUCED_LANDMARKS], y_norm[REDUCED_LANDMARKS]\n    \n    return normalized_sqc\n\nn_v = normalize_loaded_sequence(v)\nprint(n_v.shape)\nprint(np.max(n_v[0,:,0]), np.min(n_v[0,:,0]))\n\n# %% [markdown]\n# **Note** at this step I have a normalized tensor built after loading data\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.649631Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.649975Z\",\"iopub.status.idle\":\"2024-06-13T18:23:46.655957Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.649946Z\",\"shell.execute_reply\":\"2024-06-13T18:23:46.654943Z\"}}\ndef get_data(sqc_path):\n    data = load_relevant_data_subset(sqc_path)\n    data = normalize_loaded_sequence(data)\n    return data\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:46.657395Z\",\"iopub.execute_input\":\"2024-06-13T18:23:46.658037Z\",\"iopub.status.idle\":\"2024-06-13T18:23:47.097478Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:46.658009Z\",\"shell.execute_reply\":\"2024-06-13T18:23:47.096441Z\"}}\nd = get_data(select_random_sequence())\nd.shape\n# print(vv.shape)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:47.099038Z\",\"iopub.execute_input\":\"2024-06-13T18:23:47.099497Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.141099Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:47.099448Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.140205Z\"}}\npq_path = select_random_sequence() # only first sequence of user here\ncols = ['frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\nsqc_df = pd.read_parquet(pq_path, columns=cols)\n\nvv = get_data(pq_path)\n\nn_df = normalize_sequence(sqc_df)\nframe_df0 = n_df[n_df.frame == n_df.frame.unique()[0]]\nframe_df1 = n_df[n_df.frame == n_df.frame.unique()[-1]]\n\nX0 = frame_df0.x\nY0= frame_df0.y\n\nX1 = frame_df1.x\nY1= frame_df1.y\n\nplt.figure(figsize=(8,10))\nplt.subplot(1,2,1)\nplt.scatter(X0,-Y0)\nplt.scatter(vv[0,:,0],-vv[0,:,1], s=3, c='r')\n\nplt.subplot(1,2,2)\nplt.scatter(X1,-Y1)\nplt.scatter(vv[-1,:,0],-vv[-1,:,1], s=3, c='r')\n\nplt.title(pq_path)\nplt.show()\n\n# %% [markdown]\n# #### **Note** \n# Normalization using min-max change position of point when using less (but most important) landmarks, is it normal as we used less points.\n# But movement keep the same\n# \n# - RNN or LSTM can be a good simple approach for starting (it can be adapted for Time Series)\n\n# %% [markdown]\n# #### **TODO**\n# * thing about data augmentation\n# * try to use coatnet -> need to input data with same shape\n# * padding ?\n#     - issue with padding is that we have sequence with much more frames than other, maybe reduce thoses sequences and padding for small sequences\n#     - goal: have se\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.142350Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.142610Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.360685Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.142585Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.359722Z\"}}\ntrain_path = '/kaggle/input/asl-signs/train.csv'\ntrain = pd.read_csv(train_path)\ntrain.head()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.361864Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.362141Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.368127Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.362116Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.367186Z\"}}\nlen(train)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.369301Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.369607Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.379806Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.369581Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.378795Z\"}}\ntrain.columns\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.381148Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.381496Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.402384Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.381454Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.401316Z\"}}\ntrain.sign.unique()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.403472Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.403767Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.411960Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.403742Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.410876Z\"}}\ntrain.participant_id.unique(), len(train.participant_id.unique())\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.413213Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.413540Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.489303Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.413514Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.488148Z\"}}\nd=dict(train.sign.value_counts(dropna=True))\nprint(train.sign.value_counts(dropna=True).mean())\nprint(train.sign.value_counts(dropna=True).std())\nprint(train.sign.value_counts(dropna=True).max())\nprint(train.sign.value_counts(dropna=True).min())\n\n# word distribution is not too expended\n# any words have close occurences\n\n# %% [markdown]\n# #### **Some notes:**\n# * each parquet contains markers position [x y z] and type (face, left_hand, pose, right_hand) for different frame\n# * train dataset is composed of image path, participant id (folder name of parquet file) sequence id (filename) and word said\n# * one sequence = numerous frames = 1 word\n# * every frame has data for each type, but it is possible that one type has no value in a frame, it is setted to NaN\n# \n# **Goal**: using hand position, be able to understand word said in the sequence\n# * classification between 250 words using positions of body parts in video\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.490528Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.490824Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.506588Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.490798Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.505710Z\"}}\nimport json\n \n# Opening JSON file\nf = open('/kaggle/input/asl-signs/sign_to_prediction_index_map.json')\n \n# returns JSON object as \n# a dictionary\nWORD2IDX = json.load(f)\nprint(len(WORD2IDX), WORD2IDX)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.507794Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.509502Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.523368Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.509465Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.522395Z\"}}\ntrain_words = train.sign.unique()\nprint(len(train_words))\n# same length as sign to prediction index json\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.524580Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.524873Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.538715Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.524849Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.537684Z\"}}\nrandom_word = random.choice(train.sign.unique())\nprint(f'idx for <{random_word}> is <{WORD2IDX[random_word]}>')\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.539971Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.540339Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.549169Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.540306Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.548207Z\"}}\ntrain.path\n\n# %% [markdown]\n# ### Custom Dataset class\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.550473Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.550779Z\",\"iopub.status.idle\":\"2024-06-13T18:23:48.558761Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.550755Z\",\"shell.execute_reply\":\"2024-06-13T18:23:48.557776Z\"}}\nall_sqc_path = train.path\nprint(len(all_sqc_path))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:48.565412Z\",\"iopub.execute_input\":\"2024-06-13T18:23:48.565801Z\",\"iopub.status.idle\":\"2024-06-13T18:23:49.865139Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:48.565773Z\",\"shell.execute_reply\":\"2024-06-13T18:23:49.864098Z\"}}\nmyList = []\n\nfor i in range(5):\n    sq1 = select_random_sequence()\n    word = train[train.path == sq1[24:]].sign.values[0]\n    mydata = get_data(sq1)\n    print(mydata.shape)\n    myList.append((mydata, word))\n\n# %% [markdown]\n# ### **Don't run following cell, it creates *preprocess_dataset***\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:49.866573Z\",\"iopub.execute_input\":\"2024-06-13T18:23:49.866959Z\",\"iopub.status.idle\":\"2024-06-13T18:23:49.873344Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:49.866929Z\",\"shell.execute_reply\":\"2024-06-13T18:23:49.872286Z\"}}\n# Do not run !\n\n# processed_dataset = []\n# for idx,path in enumerate(train.path):\n#     sequence_path = os.path.join(COMPETITION_PATH, path)\n#     word = train[train.path == path].sign.values[0]\n#     processed_sqc = get_data(sequence_path)\n    \n#     processed_dataset.append((processed_sqc, word))\n    \n#     if idx%200 == 0:\n#         print(processed_sqc.shape, word)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:49.874532Z\",\"iopub.execute_input\":\"2024-06-13T18:23:49.874826Z\",\"iopub.status.idle\":\"2024-06-13T18:23:49.882129Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:49.874799Z\",\"shell.execute_reply\":\"2024-06-13T18:23:49.881276Z\"}}\n# to save dataset\n# with open(\"preprocess_dataset.pkl\", \"wb\") as fp:   #Pickling\n#     pickle.dump(processed_dataset, fp)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:23:49.883118Z\",\"iopub.execute_input\":\"2024-06-13T18:23:49.883608Z\",\"iopub.status.idle\":\"2024-06-13T18:24:31.376143Z\",\"shell.execute_reply.started\":\"2024-06-13T18:23:49.883583Z\",\"shell.execute_reply\":\"2024-06-13T18:24:31.375341Z\"}}\n# to load dataset\nwith open(PROCESS_DATASET_PATH, \"rb\") as fp:   # Unpickling\n    dataset = pickle.load(fp)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:31.377291Z\",\"iopub.execute_input\":\"2024-06-13T18:24:31.377581Z\",\"iopub.status.idle\":\"2024-06-13T18:24:31.386194Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:31.377556Z\",\"shell.execute_reply\":\"2024-06-13T18:24:31.385238Z\"}}\ndataset[0][1]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:31.387467Z\",\"iopub.execute_input\":\"2024-06-13T18:24:31.387762Z\",\"iopub.status.idle\":\"2024-06-13T18:24:31.394610Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:31.387730Z\",\"shell.execute_reply\":\"2024-06-13T18:24:31.393717Z\"}}\nlen(dataset)\n\n# %% [markdown]\n# ### **Custom class and Dataloader**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:31.395832Z\",\"iopub.execute_input\":\"2024-06-13T18:24:31.396175Z\",\"iopub.status.idle\":\"2024-06-13T18:24:33.966680Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:31.396149Z\",\"shell.execute_reply\":\"2024-06-13T18:24:33.965798Z\"}}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:33.967936Z\",\"iopub.execute_input\":\"2024-06-13T18:24:33.968547Z\",\"iopub.status.idle\":\"2024-06-13T18:24:33.977090Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:33.968511Z\",\"shell.execute_reply\":\"2024-06-13T18:24:33.976096Z\"}}\nclass ISLR(Dataset):\n    def __init__(self, dataset, split):\n        self.split = split\n        self.dataset = dataset\n        \n        if split == 'trainval':\n            self.islr_dataset = dataset[:int(0.8*len(dataset))]\n        elif split =='test':\n            self.islr_dataset = dataset[int(0.8*len(dataset)):]\n        \n    def __len__(self):\n        return len(self.islr_dataset)\n    \n    def __getitem__(self, index):\n        sample = self.islr_dataset[index]\n        features = torch.FloatTensor(sample[0])\n        target = WORD2IDX[sample[1]]\n        \n        return features, target\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:33.978682Z\",\"iopub.execute_input\":\"2024-06-13T18:24:33.978958Z\",\"iopub.status.idle\":\"2024-06-13T18:24:33.994285Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:33.978934Z\",\"shell.execute_reply\":\"2024-06-13T18:24:33.993390Z\"}}\ntestset = ISLR(dataset, split='test')\ntrainvalset = ISLR(dataset, split='trainval')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:33.995512Z\",\"iopub.execute_input\":\"2024-06-13T18:24:33.995839Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.138883Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:33.995785Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.137821Z\"}}\ntrainset, valset = train_test_split(trainvalset,test_size=0.1, random_state=42)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.140143Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.140484Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.147401Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.140457Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.146409Z\"}}\nlen(trainset),len(valset)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.148824Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.149112Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.157350Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.149088Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.156315Z\"}}\nlen(dataset)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.158508Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.158772Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.168105Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.158749Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.167082Z\"}}\nlen(testset)+len(trainset)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.169211Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.169530Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.176992Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.169506Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.176157Z\"}}\n# batch =[[torch.tensor([1833, 3205,  467,  342, 4165,   31, 49,  803]), torch.tensor([1])],\n#         [torch.tensor([1833, 3205,  467,  342, 49,  803]), torch.tensor([2])],\n#         [torch.tensor([1833, 3205,  467,  342, 4165,   31, 49,  803,52,54]), torch.tensor([1])]]\n# def custom_collate(batch):\n#     padded_batch=[]\n#     labels=[]\n#     for sentence,label in batch:\n#         # print(sentence.tolist())\n\n#         listSentence = sentence.tolist()\n#         max_len = max(len(sentence.tolist()) for sentence,label in batch)\n#         # print(listSentence)\n#         padded_sentence=listSentence+[5001]*(max_len-len(listSentence))\n#         # print(max_len)\n#         padded_batch.append(padded_sentence)\n#         labels.append(label)\n\n#     return torch.tensor(padded_batch), torch.tensor(labels)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.178346Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.178660Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.186530Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.178637Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.185651Z\"}}\n\ndef custom_collate_fn(batch):\n    padded_batch = []\n    labels= []\n\n    max_frame = max(len(sequence) for sequence,_ in batch)\n#     print(max_frame)\n    for sequence, label in batch:\n        padding_array = -np.ones(((max_frame-len(sequence)), len(REDUCED_LANDMARKS), 2))\n        padded_sequence = sequence.tolist()+padding_array.tolist()\n\n        padded_batch.append(padded_sequence)\n        labels.append(label)\n\n\n    return torch.tensor(padded_batch), torch.tensor(labels)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.187588Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.187829Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.199162Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.187808Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.198257Z\"}}\ntrain_loader = DataLoader(trainset, batch_size=32, collate_fn=custom_collate_fn, shuffle=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.200419Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.201152Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.207868Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.201127Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.207085Z\"}}\nval_loader = DataLoader(valset, batch_size=32, collate_fn=custom_collate_fn, shuffle=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.209209Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.209515Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.218953Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.209491Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.217969Z\"}}\ntest_loader = DataLoader(testset, batch_size=32, collate_fn=custom_collate_fn, shuffle=False)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.219984Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.220289Z\",\"iopub.status.idle\":\"2024-06-13T18:24:37.240278Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.220263Z\",\"shell.execute_reply\":\"2024-06-13T18:24:37.239456Z\"}}\ncustom_it = enumerate(train_loader)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:37.241343Z\",\"iopub.execute_input\":\"2024-06-13T18:24:37.241660Z\",\"iopub.status.idle\":\"2024-06-13T18:24:38.732172Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:37.241635Z\",\"shell.execute_reply\":\"2024-06-13T18:24:38.731198Z\"}}\nidx,(sqc,lb)=next(custom_it)\nprint(sqc.shape, lb)\n\n# %% [markdown]\n# ### **Model architecture**\n\n# %% [markdown]\n# ### Transformer\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:38.733449Z\",\"iopub.execute_input\":\"2024-06-13T18:24:38.733754Z\",\"iopub.status.idle\":\"2024-06-13T18:24:38.740098Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:38.733727Z\",\"shell.execute_reply\":\"2024-06-13T18:24:38.739235Z\"}}\n# class SignLanguageModel(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n#         super(SignLanguageModel, self).__init__()\n#         self.num_layers = num_layers\n#         self.hidden_dim = hidden_dim\n#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n#         self.fc = nn.Linear(hidden_dim, output_dim)\n\n#     def forward(self, x):\n#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n#         out, _ = self.lstm(x, (h0, c0))\n#         out = self.fc(out[:, -1, :])\n#         return out\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:24:38.741366Z\",\"iopub.execute_input\":\"2024-06-13T18:24:38.741672Z\",\"iopub.status.idle\":\"2024-06-13T18:24:38.758585Z\",\"shell.execute_reply.started\":\"2024-06-13T18:24:38.741647Z\",\"shell.execute_reply\":\"2024-06-13T18:24:38.757762Z\"}}\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, output_dim, n_landmarks, max_seq_length=1000):\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Linear(input_dim*n_landmarks, hidden_dim) # change encoding \n        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n        self.positional_encoding = self.positional_encoding = self.create_positional_encoding(max_seq_length, hidden_dim)\n        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        # activation softmax\n        \n        \n    def forward(self, x):\n        \n        batch_size, n_frames, n_landmarks, input_dim = x.shape\n        pad_mask = self.sequence_mask(x)\n        pad_mask = pad_mask.to(device)\n        \n        \n        # Flatten n_landmarks and input_dim for embedding\n        x = x.view(batch_size, n_frames, -1)\n        x = x.to(device)\n        x = self.embedding(x)\n        \n        x = self.layer_norm1(x)\n        x += self.positional_encoding[:, :n_frames, :].to(device)\n        x = x.permute(1, 0, 2)  # Transformer expects sequence length first\n                \n        transformer_out = self.transformer_encoder(x,src_key_padding_mask=pad_mask)\n        out = self.fc(transformer_out[-1, :, :])\n        assert not torch.isnan(out).any(), \"NaN in final output\"\n        \n        \n        return out\n    \n    def create_positional_encoding(self, max_seq_length, hidden_dim):\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n        \n        positional_encoding = torch.zeros(max_seq_length, hidden_dim)\n        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n        \n        return positional_encoding.unsqueeze(0)\n    \n    def sequence_mask(self, sequence):\n        lengths = [self.valid_len(padded_sequence) for padded_sequence in sequence]\n        \n        mask = torch.zeros(sequence.size()[:2], dtype=torch.bool)  # shape: [batch_size, n_frames]\n        for i, length in enumerate(lengths):\n            mask[i, :length] = 1\n        \n        mask = ~mask # True values are ignored\n        return mask\n\n        \n    def valid_len(self, padded_sequence):\n        for idx, frame in  enumerate(padded_sequence):\n            if -1 in frame:\n                break\n\n        return idx+1\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:28:45.976373Z\",\"iopub.execute_input\":\"2024-06-13T18:28:45.976748Z\",\"iopub.status.idle\":\"2024-06-13T18:28:45.998483Z\",\"shell.execute_reply.started\":\"2024-06-13T18:28:45.976718Z\",\"shell.execute_reply\":\"2024-06-13T18:28:45.997447Z\"}}\n# Exemple d'utilisation\ninput_dim = 2  # (x, y)\nnum_heads = 4 # attention head == later\nnum_layers = 2 # \nhidden_dim = 64\noutput_dim = 250  # nombre de mots\nn_landmarks = 92\n\n# vision transformer = find similar project\n# try CNN \n# point Net\n# LSTM + conv \n\nmodel = TransformerModel(input_dim=input_dim,\n                         num_heads=num_heads,\n                         num_layers=num_layers,\n                         hidden_dim=hidden_dim,\n                         output_dim=output_dim,\n                         n_landmarks=n_landmarks)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\nprint(model)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:26:14.342175Z\",\"iopub.execute_input\":\"2024-06-13T18:26:14.343005Z\",\"iopub.status.idle\":\"2024-06-13T18:26:14.384283Z\",\"shell.execute_reply.started\":\"2024-06-13T18:26:14.342974Z\",\"shell.execute_reply\":\"2024-06-13T18:26:14.383412Z\"}}\nout  = model(sqc)\n# [torch.argmax(vi, ) for vi in v]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:27:24.631778Z\",\"iopub.execute_input\":\"2024-06-13T18:27:24.632171Z\",\"iopub.status.idle\":\"2024-06-13T18:27:24.686418Z\",\"shell.execute_reply.started\":\"2024-06-13T18:27:24.632139Z\",\"shell.execute_reply\":\"2024-06-13T18:27:24.685382Z\"}}\nk_pred, idx = torch.topk(out , 5, dim=1)\nnp.array([l in id for l in lb for id in idx]).sum()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.724617Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.724985Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.732209Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.724954Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.730615Z\"}}\ndef valid_len(padded_sequence):\n    for idx, frame in  enumerate(padded_sequence):\n        if -1 in frame:\n            break\n    \n    return idx+1\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.733881Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.734432Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.751764Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.734386Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.750093Z\"}}\nsqc.shape\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.753651Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.754173Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.782379Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.754126Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.781095Z\"}}\nlengths = [valid_len(padded_sequence) for padded_sequence in sqc]\nlengths\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.783721Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.784085Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.792834Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.784047Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.791616Z\"}}\nmask = torch.zeros(sqc.size()[:2], dtype=torch.bool)  # shape: [batch_size, n_frames, 1]\nprint(mask.shape)\nfor i, length in enumerate(lengths):\n    mask[i, :length] = 1\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.794323Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.794755Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.811892Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.794715Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.810582Z\"}}\nmask[1]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.813465Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.813828Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.824602Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.813799Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.823305Z\"}}\n# mask = mask.sum(dim=1) == 0\nmask = ~mask\nmask[1]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-07T06:43:27.826099Z\",\"iopub.execute_input\":\"2024-06-07T06:43:27.826503Z\",\"iopub.status.idle\":\"2024-06-07T06:43:27.836716Z\",\"shell.execute_reply.started\":\"2024-06-07T06:43:27.826469Z\",\"shell.execute_reply\":\"2024-06-07T06:43:27.835355Z\"}}\nmask.shape\n\n# %% [markdown]\n# ### 1D CNN\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:39:17.581451Z\",\"iopub.execute_input\":\"2024-06-12T12:39:17.582191Z\",\"iopub.status.idle\":\"2024-06-12T12:39:17.593291Z\",\"shell.execute_reply.started\":\"2024-06-12T12:39:17.582158Z\",\"shell.execute_reply\":\"2024-06-12T12:39:17.592356Z\"}}\nclass SignLanguageCNN1D(nn.Module):\n    def __init__(self, n_landmarks, input_dim, num_classes):\n        super(SignLanguageCNN1D, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(256 * n_landmarks, 512)  # Note: Adjusted for dynamic sequence length\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        batch_size, n_frames, n_landmarks, input_dim = x.size()\n        x = x.permute(0, 3, 1, 2)  # Change from [batch_size, n_frames, n_landmarks, input_dim] to [batch_size, input_dim, n_frames, n_landmarks]\n        x = x.reshape(batch_size * n_landmarks, input_dim, n_frames)  # Combine batch and landmarks for 1D conv\n        x = self.pool(F.relu(self.conv1(x)))  # Output shape: [batch_size * n_landmarks, 64, n_frames//2]\n        x = self.pool(F.relu(self.conv2(x)))  # Output shape: [batch_size * n_landmarks, 128, n_frames//4]\n        x = self.pool(F.relu(self.conv3(x)))  # Output shape: [batch_size * n_landmarks, 256, n_frames//8]\n        x = self.adaptive_pool(x)  # Output shape: [batch_size * n_landmarks, 256, 1]\n        x = x.view(batch_size, n_landmarks, -1)  # Reshape back to [batch_size, n_landmarks, feature_dim]\n#         print(x.shape)\n        x = x.view(batch_size, -1)  # Flatten to [batch_size, n_landmarks * feature_dim]\n#         print(x.shape)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n        \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:56:38.355386Z\",\"iopub.execute_input\":\"2024-06-12T12:56:38.355775Z\",\"iopub.status.idle\":\"2024-06-12T12:56:38.362123Z\",\"shell.execute_reply.started\":\"2024-06-12T12:56:38.355735Z\",\"shell.execute_reply\":\"2024-06-12T12:56:38.361119Z\"}}\nn_landmarks = dataset[0][0].shape[1]\ninput_dim = dataset[0][0].shape[2]\nnum_classes = 250\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:56:38.690956Z\",\"iopub.execute_input\":\"2024-06-12T12:56:38.691274Z\",\"iopub.status.idle\":\"2024-06-12T12:56:38.696290Z\",\"shell.execute_reply.started\":\"2024-06-12T12:56:38.691249Z\",\"shell.execute_reply\":\"2024-06-12T12:56:38.695339Z\"}}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:56:39.308748Z\",\"iopub.execute_input\":\"2024-06-12T12:56:39.309077Z\",\"iopub.status.idle\":\"2024-06-12T12:56:39.420364Z\",\"shell.execute_reply.started\":\"2024-06-12T12:56:39.309049Z\",\"shell.execute_reply\":\"2024-06-12T12:56:39.419552Z\"}}\nmodel = SignLanguageCNN1D(n_landmarks, input_dim, num_classes).to(device)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:39:20.886880Z\",\"iopub.execute_input\":\"2024-06-12T12:39:20.887201Z\",\"iopub.status.idle\":\"2024-06-12T12:39:22.434172Z\",\"shell.execute_reply.started\":\"2024-06-12T12:39:20.887173Z\",\"shell.execute_reply\":\"2024-06-12T12:39:22.433157Z\"}}\nout = model(sqc)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-12T12:35:14.279922Z\",\"iopub.execute_input\":\"2024-06-12T12:35:14.280848Z\",\"iopub.status.idle\":\"2024-06-12T12:35:14.290818Z\",\"shell.execute_reply.started\":\"2024-06-12T12:35:14.280799Z\",\"shell.execute_reply\":\"2024-06-12T12:35:14.289901Z\"}}\n# torch.max(out, 1)\n\n# %% [markdown]\n# ### **Training Phase**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:29:02.337442Z\",\"iopub.execute_input\":\"2024-06-13T18:29:02.338206Z\",\"iopub.status.idle\":\"2024-06-13T18:29:03.816753Z\",\"shell.execute_reply.started\":\"2024-06-13T18:29:02.338169Z\",\"shell.execute_reply\":\"2024-06-13T18:29:03.815751Z\"}}\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:29:05.720118Z\",\"iopub.execute_input\":\"2024-06-13T18:29:05.721118Z\",\"iopub.status.idle\":\"2024-06-13T23:29:02.946524Z\",\"shell.execute_reply.started\":\"2024-06-13T18:29:05.721082Z\",\"shell.execute_reply\":\"2024-06-13T23:29:02.945557Z\"}}\nnum_epochs = 5\n\ndataloader = train_loader\n\nfor epoch in range(num_epochs):\n\n    print(f'Epoch {epoch}/{num_epochs - 1}')\n    print('-' * 10)\n    \n    model.train()\n    running_loss = 0.0\n\n    for sequence, label in dataloader:\n        sequence, label = sequence.to(device), label.to(device)\n        optimizer.zero_grad()\n\n        target = label\n        \n        outputs = model(sequence)\n\n#         predictions = torch.argmax(outputs, dim=1) # get index of max word\n\n        # Compute the loss, gradients, and update optimizer\n        loss = loss_function(outputs, target)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n#         running_corrects += torch.sum(predictions == label)\n\n#     exp_lr_scheduler.step()\n\n    epoch_loss = running_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n    \n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for sequence, label in val_loader:\n            sequence, label = sequence.to(device), label.to(device)\n            outputs = model(sequence)\n            loss = loss_function(outputs, label)\n            val_loss += loss.item() * sequence.size(0)\n            \n#           use top 5 pred maybe\n            _, predicted = torch.topk(outputs, 5, dim=1)\n\n#             _, predicted = torch.max(outputs, 1)\n            total += label.size(0)\n            correct += np.array([lab in pred for lab in label for pred in predicted]).sum()            \n#             correct += (predicted == label).sum().item()\n            \n    val_loss /= len(val_loader.dataset)\n    val_acc = correct /total\n    print(f'val loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n    \n    torch.save(model.state_dict(), f'pointnet_transformer_model_{epoch+1}.pth')\n\n# %% [code]\n'''\nEpoch 0/4\n----------\nEpoch 1/5, Training Loss: 5.5489\nval loss: 5.5252 Acc: 0.6634\nEpoch 1/4\n----------\nEpoch 2/5, Training Loss: 5.5313\nval loss: 5.5213 Acc: 0.6592\nEpoch 2/4\n----------\nEpoch 3/5, Training Loss: 5.4685\nval loss: 5.3126 Acc: 0.7019\nEpoch 3/4\n----------\nEpoch 4/5, Training Loss: 5.0170\nval loss: 4.6766 Acc: 0.8116\nEpoch 4/4\n----------\nEpoch 5/5, Training Loss: 4.4992\nval loss: 4.2321 Acc: 0.9436\n'''\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T17:58:31.111522Z\",\"iopub.execute_input\":\"2024-06-13T17:58:31.112596Z\",\"iopub.status.idle\":\"2024-06-13T17:58:31.119264Z\",\"shell.execute_reply.started\":\"2024-06-13T17:58:31.112556Z\",\"shell.execute_reply\":\"2024-06-13T17:58:31.118379Z\"}}\n'''\nEpoch 0/4\n----------\nEpoch 1/5, Training Loss: 5.6811\nval loss: 5.6895 Acc: 0.0030\nEpoch 1/4\n----------\nEpoch 2/5, Training Loss: 5.6814\nval loss: 5.6895 Acc: 0.0030\nEpoch 2/4\n----------\n'''\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:11:26.076636Z\",\"iopub.execute_input\":\"2024-06-13T18:11:26.077340Z\",\"iopub.status.idle\":\"2024-06-13T18:11:26.112964Z\",\"shell.execute_reply.started\":\"2024-06-13T18:11:26.077308Z\",\"shell.execute_reply\":\"2024-06-13T18:11:26.111723Z\"}}\n_, p = torch.topk(outputs, 5, dim=1)\n\n# %% [code]\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-13T18:11:52.207489Z\",\"iopub.execute_input\":\"2024-06-13T18:11:52.207869Z\",\"iopub.status.idle\":\"2024-06-13T18:11:52.375122Z\",\"shell.execute_reply.started\":\"2024-06-13T18:11:52.207839Z\",\"shell.execute_reply\":\"2024-06-13T18:11:52.374162Z\"}}\n\n\n# %% [markdown]\n# #### **Analysis Ideas**\n# \n# * class embalencement (count words for each element in train dataset)\n# * size analysis (lenght of sequence, linked to words ?)\n# * position ranges (x y z)\n# * number of sequence per participant \n# * train dataset will be splitted for train test val\n\n# %% [code]\n\n\n# %% [code]\n","metadata":{"_uuid":"a405ceb6-b385-4ca8-a981-6a02ef7d089b","_cell_guid":"660c1c76-040a-4ef1-84e6-3596e416d3e7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}